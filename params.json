{"tagline":"A robots.txt parser written in PHP, useful for checking whether or not your bot has access to a given URL.","note":"Don't delete this file! It's used internally to help with page regeneration.","body":"/// copyleft 2012 meklu (public domain)\r\n\r\nrbt_prs is a robots.txt parser written in PHP and intended for checking\r\nwhether or not your bot can access a given URL.\r\n\r\nCurrently it provides just one function:\r\n\r\n`\r\n\tisUrlBotSafe($url, $your_useragent, $robots_txt, $redirects, $debug)\r\n`\r\n\r\nSetting any of the last four parameters to NULL will make them use their\r\ndefault values.\r\n\r\nThe first argument, $url, is the only required argument and means the URL you\r\nwould like to check.\r\n\r\nThe second argument, $your_useragent, is the user agent string your bot will\r\nuse. While it isn't mandatory to select one, it is highly advised. To mention\r\nthis software in your user agent string, you could invoke the function as\r\n\r\n`\r\n\tisUrlBotSafe(\"http://example.com/\", \"Fancybot/0.1 (awesome) \" .\r\n\t\t     RBT_PRS_UA);\r\n`\r\n\r\nNotice the space after the description. The constant RBT_PRS_UA doesn't have\r\none in it.\r\n\r\nThe third argument, $robots_txt, is useful when you want to go through several\r\nURLs with just a single download of robots.txt. It allows you to use your own\r\nrules on a given URL. You could utilise the argument like this:\r\n\r\n`\r\n\t$myrules = file_get_contents(\"http://example.com/robots.txt\");\r\n\tisUrlBotSafe(\"http://example.com/\", \"Fancybot/0.1\", $myrules);\r\n\tisUrlBotSafe(\"http://example.com/contact\", \"Fancybot/0.1\", $myrules);\r\n\tisUrlBotSafe(\"http://example.com/blog\", \"Fancybot/0.1\", $myrules);\r\n\tisUrlBotSafe(\"http://example.com/about\", \"Fancybot/0.1\", $myrules);\r\n`\r\n\r\nThe fourth argument, $redirects, is only useful when you let the function\r\ndownload the robots.txt. It allows you to handle HTTP redirects in a slightly\r\nbetter way than having to miserably fail when grabbing the file.\r\nValues that are 1 or less will disable redirects, whereas FALSE will leave\r\nthings the way they are. TRUE will set the value to 20. Here's how to use a\r\nsimilar approach with $robots_txt defined.\r\n\r\n`\r\n\t$opts = array('http' => array('method' => 'GET',\r\n\t\t\t\t      'max_redirects' => 20)\r\n\t\t     );\r\n\t$context = stream_context_create($opts);\r\n\t$myrules = file_get_contents(\"http://example.com/robots.txt\", FALSE,\r\n\t\t\t\t     $context);\r\n`\r\n\r\nThe fifth argument, $debug, will make the function verbose and have it print\r\nall sorts of useful analysis on the rules.\r\n\r\nThis piece of software is already in use in real life at\r\nhttp://cheesetalks.twolofbees.com/humble/ and was originally written for the\r\nsite author just because it was the correct way of doing things.\r\n","google":"","name":"rbt_prs"}